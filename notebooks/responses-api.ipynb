{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf3f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "## NOTE: Semantick Kernel needs AsyncOpenAI client, OpenAI client is not supported.\n",
    "client = AsyncOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_REASONING_API_KEY\"),\n",
    "    base_url=\"https://lseg-foundry-demo.cognitiveservices.azure.com/openai/v1/\", ## https://<just-replace-this>.cognitiveservices.azure.com/openai/v1/\n",
    "    default_query={\"api-version\": \"preview\"}, \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ade5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"resp_689dd1bd3cf48190a3ace3061e4729910fe78ea1ef1ba35c\",\n",
      "  \"created_at\": 1755173310.0,\n",
      "  \"error\": null,\n",
      "  \"incomplete_details\": null,\n",
      "  \"instructions\": null,\n",
      "  \"metadata\": {},\n",
      "  \"model\": \"o4-mini\",\n",
      "  \"object\": \"response\",\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"id\": \"rs_689dd219e03081908ff536d25005610e0fe78ea1ef1ba35c\",\n",
      "      \"summary\": [],\n",
      "      \"type\": \"reasoning\",\n",
      "      \"content\": null,\n",
      "      \"encrypted_content\": null,\n",
      "      \"status\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"msg_689dd2228c6c819082a78f739380e3bb0fe78ea1ef1ba35c\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"annotations\": [],\n",
      "          \"text\": \"The capital of France is Paris.\",\n",
      "          \"type\": \"output_text\",\n",
      "          \"logprobs\": null\n",
      "        }\n",
      "      ],\n",
      "      \"role\": \"assistant\",\n",
      "      \"status\": \"completed\",\n",
      "      \"type\": \"message\"\n",
      "    }\n",
      "  ],\n",
      "  \"parallel_tool_calls\": true,\n",
      "  \"temperature\": 1.0,\n",
      "  \"tool_choice\": \"auto\",\n",
      "  \"tools\": [],\n",
      "  \"top_p\": 1.0,\n",
      "  \"background\": false,\n",
      "  \"max_output_tokens\": null,\n",
      "  \"max_tool_calls\": null,\n",
      "  \"previous_response_id\": null,\n",
      "  \"prompt\": null,\n",
      "  \"prompt_cache_key\": null,\n",
      "  \"reasoning\": {\n",
      "    \"effort\": \"low\",\n",
      "    \"generate_summary\": null,\n",
      "    \"summary\": null\n",
      "  },\n",
      "  \"safety_identifier\": null,\n",
      "  \"service_tier\": \"default\",\n",
      "  \"status\": \"completed\",\n",
      "  \"text\": {\n",
      "    \"format\": {\n",
      "      \"type\": \"text\"\n",
      "    },\n",
      "    \"verbosity\": null\n",
      "  },\n",
      "  \"top_logprobs\": null,\n",
      "  \"truncation\": \"disabled\",\n",
      "  \"usage\": {\n",
      "    \"input_tokens\": 13,\n",
      "    \"input_tokens_details\": {\n",
      "      \"cached_tokens\": 0\n",
      "    },\n",
      "    \"output_tokens\": 13,\n",
      "    \"output_tokens_details\": {\n",
      "      \"reasoning_tokens\": 0\n",
      "    },\n",
      "    \"total_tokens\": 26\n",
      "  },\n",
      "  \"user\": null,\n",
      "  \"content_filters\": null,\n",
      "  \"store\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = await client.responses.create(   \n",
    "  model=\"o4-mini\", # Replace with your **model deployment name**\n",
    "  input=f\"whats the capital of france?\",\n",
    "  reasoning={\n",
    "    \"effort\": \"low\",  # Options: low, medium, high\n",
    "    # \"summary\": \"detailed\" # One of auto, concise, or detailed\n",
    "  },\n",
    "  # previous_response_id=response.id,  # Use the previous response ID if you want to continue the conversation\n",
    ")\n",
    "\n",
    "print(response.model_dump_json(indent=2))\n",
    "print(response.output[-1].content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba336db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "print(response.output[-1].content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c2c7bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"resp_688ca18a87688190915367e1d366a2a70490b7a3507a39fa\",\n",
      "  \"created_at\": 1754046858.0,\n",
      "  \"error\": null,\n",
      "  \"incomplete_details\": null,\n",
      "  \"instructions\": null,\n",
      "  \"metadata\": {},\n",
      "  \"model\": \"o4-mini\",\n",
      "  \"object\": \"response\",\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"id\": \"rs_688ca18af8848190b461e1751e4f2ad10490b7a3507a39fa\",\n",
      "      \"summary\": [],\n",
      "      \"type\": \"reasoning\",\n",
      "      \"encrypted_content\": null,\n",
      "      \"status\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"msg_688ca18b95e88190bb49444ba5d4e5980490b7a3507a39fa\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"annotations\": [],\n",
      "          \"text\": \"The capital of France is Paris.\",\n",
      "          \"type\": \"output_text\",\n",
      "          \"logprobs\": null\n",
      "        }\n",
      "      ],\n",
      "      \"role\": \"assistant\",\n",
      "      \"status\": \"completed\",\n",
      "      \"type\": \"message\"\n",
      "    }\n",
      "  ],\n",
      "  \"parallel_tool_calls\": true,\n",
      "  \"temperature\": 1.0,\n",
      "  \"tool_choice\": \"auto\",\n",
      "  \"tools\": [],\n",
      "  \"top_p\": 1.0,\n",
      "  \"background\": false,\n",
      "  \"max_output_tokens\": null,\n",
      "  \"max_tool_calls\": null,\n",
      "  \"previous_response_id\": null,\n",
      "  \"prompt\": null,\n",
      "  \"reasoning\": {\n",
      "    \"effort\": \"high\",\n",
      "    \"generate_summary\": null,\n",
      "    \"summary\": null\n",
      "  },\n",
      "  \"service_tier\": \"default\",\n",
      "  \"status\": \"completed\",\n",
      "  \"text\": {\n",
      "    \"format\": {\n",
      "      \"type\": \"text\"\n",
      "    }\n",
      "  },\n",
      "  \"top_logprobs\": null,\n",
      "  \"truncation\": \"disabled\",\n",
      "  \"usage\": {\n",
      "    \"input_tokens\": 13,\n",
      "    \"input_tokens_details\": {\n",
      "      \"cached_tokens\": 0\n",
      "    },\n",
      "    \"output_tokens\": 77,\n",
      "    \"output_tokens_details\": {\n",
      "      \"reasoning_tokens\": 64\n",
      "    },\n",
      "    \"total_tokens\": 90\n",
      "  },\n",
      "  \"user\": null,\n",
      "  \"content_filters\": null,\n",
      "  \"prompt_cache_key\": null,\n",
      "  \"safety_identifier\": null,\n",
      "  \"store\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "## NOTE: Semantick Kernel needs AsyncOpenAI client, OpenAI client is not supported.\n",
    "client = AsyncOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_REASONING_API_KEY\"),\n",
    "    base_url=\"https://lseg-foundry-demo.cognitiveservices.azure.com/openai/v1/\", ## https://<just-replace-this>.cognitiveservices.azure.com/openai/v1/\n",
    "    default_query={\"api-version\": \"preview\"}, \n",
    ")\n",
    "\n",
    "response = await client.responses.create(   \n",
    "  model=\"o4-mini\", # Replace with your **model deployment name**\n",
    "  input=\"What is the capital of France.\",\n",
    "  reasoning={\n",
    "    \"effort\": \"high\",  # Options: low, medium, high\n",
    "    # \"summary\": \"auto\" # One of auto, concise, or detailed # GATED: Needs access.\n",
    "  }\n",
    ")\n",
    "\n",
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad7ddf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"resp_688c9e9798ec8190adabc27e10a61e2208f87bfa8716ad92\",\n",
      "  \"created_at\": 1754046103.0,\n",
      "  \"error\": null,\n",
      "  \"incomplete_details\": null,\n",
      "  \"instructions\": null,\n",
      "  \"metadata\": {},\n",
      "  \"model\": \"o4-mini\",\n",
      "  \"object\": \"response\",\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"id\": \"rs_688c9e9879688190ade3d06e5ddf855c08f87bfa8716ad92\",\n",
      "      \"summary\": [],\n",
      "      \"type\": \"reasoning\",\n",
      "      \"encrypted_content\": null,\n",
      "      \"status\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"msg_688c9e9d60888190a4ac706cec61d9b408f87bfa8716ad92\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"annotations\": [],\n",
      "          \"text\": \"You actually hadn’t asked a question before this one – “This is a test.” was a statement. So this current “What was my last question?” is your first question.\",\n",
      "          \"type\": \"output_text\",\n",
      "          \"logprobs\": null\n",
      "        }\n",
      "      ],\n",
      "      \"role\": \"assistant\",\n",
      "      \"status\": \"completed\",\n",
      "      \"type\": \"message\"\n",
      "    }\n",
      "  ],\n",
      "  \"parallel_tool_calls\": true,\n",
      "  \"temperature\": 1.0,\n",
      "  \"tool_choice\": \"auto\",\n",
      "  \"tools\": [],\n",
      "  \"top_p\": 1.0,\n",
      "  \"background\": false,\n",
      "  \"max_output_tokens\": null,\n",
      "  \"max_tool_calls\": null,\n",
      "  \"previous_response_id\": \"resp_688c9e93d9c08190a796fedf536bb28108f87bfa8716ad92\",\n",
      "  \"prompt\": null,\n",
      "  \"reasoning\": {\n",
      "    \"effort\": \"high\",\n",
      "    \"generate_summary\": null,\n",
      "    \"summary\": null\n",
      "  },\n",
      "  \"service_tier\": \"default\",\n",
      "  \"status\": \"completed\",\n",
      "  \"text\": {\n",
      "    \"format\": {\n",
      "      \"type\": \"text\"\n",
      "    }\n",
      "  },\n",
      "  \"top_logprobs\": null,\n",
      "  \"truncation\": \"disabled\",\n",
      "  \"usage\": {\n",
      "    \"input_tokens\": 37,\n",
      "    \"input_tokens_details\": {\n",
      "      \"cached_tokens\": 0\n",
      "    },\n",
      "    \"output_tokens\": 682,\n",
      "    \"output_tokens_details\": {\n",
      "      \"reasoning_tokens\": 640\n",
      "    },\n",
      "    \"total_tokens\": 719\n",
      "  },\n",
      "  \"user\": null,\n",
      "  \"content_filters\": null,\n",
      "  \"prompt_cache_key\": null,\n",
      "  \"safety_identifier\": null,\n",
      "  \"store\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = await client.responses.create(   \n",
    "  model=\"o4-mini\", # Replace with your **model deployment name**\n",
    "  input=\"What was my last question.\",\n",
    "  reasoning={\n",
    "    \"effort\": \"high\",  # Options: low, medium, high\n",
    "    # \"summary\": \"auto\" # One of auto, concise, or detailed # GATED: Needs access.\n",
    "  },\n",
    "  # This is how continuation works, you can pass the previous response id to continue the conversation.\n",
    "  previous_response_id=response.id if response else None \n",
    ")\n",
    "\n",
    "print(response.model_dump_json(indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7756492e",
   "metadata": {},
   "source": [
    "# Zero Data Retention\n",
    "\n",
    "If you organization has zero-data retention, you must store the encrypted reasoning and provide it yourslef."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20508801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1 First output\n",
      "reasoning {'id': 'rs_688ca10c3f5c8190a8e18752e7c8ff6f0903d136c77f0aeb', 'summary': [], 'type': 'reasoning', 'encrypted_content': 'gAAAAABojKEQV93Twd7mGt0TCK2wKosRgXqiCo0cRlerNBdff4tHO6ehe_Kg3EUFkTk9NmvGKKggTSEqo9bzUGS3VXOpRfCAI1iyvw8l6zf4cPDkvmRVSTetriEJ-M1YovwO8hvsmIScRrAaWJwJIad15US4aKWPA4rV-4NxTSGArvhuWrdDjbV_m9b6Zr6FrrktM0OksguVMySFckSB69iAK-mkwuX1DdSPG4hCYlx8nghEYqJakkhvnudMsvjn1ZIq89wERp-fGK0RcgRt9Uqp2pXyNDsV3OziYnnIQz5O0Upv-rIWZl7cjZQ6eczgyG6YWw1hd3VUX0RNB4Y9AslaKp0OeSUhQtfVadTeoxB-OizXwjzVPVaNhcwJGbcq6byg8lB-1kHb01xhxfE0pCyjLa5cqIxDVEPVvVNxwoN-zwoWjXde-TS33yXO-WKb7yOJNfDNuxCsIlKxhagA7jNZpqWOi2ia9UThfF4ugDqUWnOqF9mIp-jt7fjlbKXPmGjcNHWudx3mCD-gQ9pe_A353y9G5_9t9sNWxEynSBi8IZ0bUcyeI6m-dDQQRBOfHHvthkiMapjhFStwnV2hjau1GSKKiptJmoNBZAagks04rqHd-4-yE3Bgale_7Q7a3EtsIAAfDSLrVGOjgWjjQkaz0GcgRone0xDrJea7f3TzhCOxjW_B5L2CNLK3z9cFRZNRG-p4Gw_U5pKmJ2CfFmEgYiLmgJllHHVrrsDoc117q--TIDnxPP6HImhmPJUN0pvMUjeP8psAmZ4LUty-CVXIiH3Xqs8oNTuPAosxvp9qiuZQVHA8VkDdh8l1hvVaMl0-_6tw5hUn2rqUCJ88It0wnSchyqYSm-y6gg2ceDFFIcfWdmO3GkPaWfDWYN1ZyswZuU7S8mCpPASRPr-lOVX4FzXUgL_Mt7RsX2ZcEOr339_e-5_rsCyzqfXRvXzQzlkTER8XG6w12K5fleetYUNYPuCSOZ4qmn8UloB5A3KHIkPzI1DVm4Pgjwrm7nBhzluokYEc3Y0an_YRnY_ybAsvOwzk3g8IHAnL8tiT_adpz-EEcYjvPcmfwG8bFfn3BW8xQ2qQ4Ssvfmtk8QDvwfVwBoCVHSFdvX3UdAD8AVHpMMwbsKMEtqheBFj7pCM2nJw_QAEBE_RMJm2sFSDHvAxUN5BPtJSGhIyXs24NiWdbxVSaEVfiWZecGXSqK5158zPyW6j6KwnZPYrAi-H5vEXltgu6ArnJ1pEqbJxFdFd7QnlXbLtoGcE_ZKeOrMKpIOERY9gFl1nKsFnaZl82oStxyVF-GUEz5CHNXBcgPYYWDmtGIcG3cIxcTsULHb5f5ZCOfj6Ojm7ukYZjUkMBUUNMYurTkiTPkNIT8aSJyJE4DnZJeOTn9TC8jspJSwaJvRgW65glyiwWvpo_yPBtmY1mNUd0bEzbKmtpIKhva1HOuzMWhq8dZXwM-qzSUOnM7P4A_pagyJ2gEvV4KCEpug4Z3D3AX_NV43AQDAfFLDga6G-bFGxSZoV40SPBtsUy29gas2S3Dmk_u8VLPoGyXJvEtAqgsnzk0iDHmrZgQgOrCYMpvniypSEkalQyWv827H-6eOa8O-h3ORSTlmM60NGEIHCAO_qyG2LsLbm2x7xFTOiXus70sXtZI8Um8LZAP2AiliIrR80q9OxuRaM97abDAHOUIPJyBpfs4DB685Y3MuxDZrWf1NVZTHinEbdHDZLkdQ-jXxPS6EgoDNmKMPv3GDBKqb4Qc_ryJXRDCstiGhSoaxcb441GO4JhRyc8GFWWS9zZj6JtEQtWTOXIfxa_AAMcsiSqoOSPGaFzA0xxfHjiCNspevzo8ZTXnnwyj5AZVIndFY-YqomgduKXp1Y-o5lYlaeK-LIPXP9LgaYWIsOeXtM0Z0vanHVrvKg8WZY_OQMKZH_Y8zIcBX6s_mX16hTC6hilvRgLPdcecyRWCbr_i1nqiT7EZpdzwQgcLNLGXuFeBwHlTTObRH5zdtblRtFgwGH9mvKxm-wO52v6X9abmvQycTM='}\n",
      "message {'id': 'msg_688ca10d9d648190b846b4e5d1cf9c8c0903d136c77f0aeb', 'content': [{'annotations': [], 'text': 'Here’s a concise, step-by-step explanation of why the daytime sky appears blue:\\n\\n1. Sunlight contains all visible colors (wavelengths) mixed together, which our eyes perceive as white light.  \\n2. As this white light enters Earth’s atmosphere, it collides with molecules of nitrogen, oxygen and tiny particles.  \\n3. Rayleigh scattering principle: the amount of light scattered by these tiny particles is inversely proportional to the fourth power of the wavelength (intensity ∝ 1/λ4).  \\n   – Shorter wavelengths (blue, ~450 nm) scatter much more strongly than longer wavelengths (red, ~650 nm).  \\n4. Scattered blue light is sent in all directions. Wherever you look away from the direct sun, your eyes pick up this preferentially scattered blue.  \\n5. At sunrise or sunset, sunlight must pass through a thicker layer of atmosphere. Most of the blue and green light is scattered out before reaching you, so the remaining direct light looks redder and orange.\\n\\nIn short, because air molecules scatter blue light far more effectively than red, the sky overhead appears blue to our eyes.', 'type': 'output_text'}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}\n",
      "#2 Second output\n",
      "reasoning {'id': 'rs_688ca110de648190a21153c603f7b11e0903d136c77f0aeb', 'summary': [], 'type': 'reasoning', 'encrypted_content': 'gAAAAABojKESZ2m4FZ4r96QPVLq4186TUpUj7icC5ujSxTU5XCjV2l2L3KMhv2SPiufY1P_TIjHzWlOXrZ1B7ULvkhtLo3gVR5f6Aj4ZAnYYmxg-nE-suASoyPMlPF5IqIHscLuK77ZVLIRZIQCc1XXvwSBiJAzIFl-nrASvetFUo3zV6yOLC4oCgwkXPC3h4OH2NKRxfx1NYnsGAYcvoa08KIUVzW0ZJN681BVEcCJh1fxyiGqjNP_1enaVdB1E7jlDXdDLMhZG4h7ggDrpl7ceEQcevU7xHVXV-89zZPhKTBQuQkpYyj7Mv2GOJy_Kwxp7src2gwTDWfjkZsMAeJOXmWIzus9sEIs9pDBSJejxpMVuE4LkycpXvmH98SaPqnLKyTjytzI290qAK1UNRTHzdidVhkHnwd4b1Ad9sA0OnZFzQtIJmYbzsfx_hytIHbEWm1OjXtIeLlNubYbih49hIruXQTR-_dURNzGyVffRBBfEmOgHzcvFOlIBmsTCkS-Mj9LUbXBYfsaLlIbOTFUUo_j8cPYmzSfBZqV_QFEINdtrJgHgWDUbzATa45So0XXuGmPxQXlc7PZYj13fwYjrvcUjkAwZkRtF0gLufEyL7ElpFSWcpbYeqyPjnloiC2FVodaTJ093qe5hSknyCnbouGpcHzBDUk29a1bbeD4BM4-qMoFAwK1A3lb-XpNrlkKq5O1b28pJ8e4itCfyCzZB32mzOzWh3BwF4-aY4jNPQYwWqti_8nzMK7rG2408cmXOALzBaKFqZkunncCadvrETemtR2k1H03GyjoMsg5DkjPsyZyr66X4rICnnOSZ5ebpi3Ct3XT7LRcpgjDRLnaEaSjxoz-fZX-TvC-cvdc3H9edXQarETh00B9l_cC0ss6ZrYX1FZvYYfFgqmCbhUR59oB7wRSCWtHA5DI4Zy_iu0tmWGUzu3A='}\n",
      "message {'id': 'msg_688ca11110c88190b216ed6e1a5752fd0903d136c77f0aeb', 'content': [{'annotations': [], 'text': 'The sky appears blue because air molecules scatter shorter-wavelength blue light from the sun more effectively than longer-wavelength red light, sending blue light in all directions.', 'type': 'output_text'}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}\n"
     ]
    }
   ],
   "source": [
    "inputs = [{  \"role\": \"user\", \"content\": \"Explain why is the sky blue.\" } ]\n",
    "\n",
    "response_1 = await client.responses.create(   \n",
    "  model=\"o4-mini\", # Replace with your **model deployment name**\n",
    "  input=inputs,\n",
    "  reasoning={\n",
    "    \"effort\": \"high\",  # Options: low, medium, high\n",
    "    # \"summary\": \"auto\" # One of auto, concise, or detailed # GATED: Needs access.\n",
    "  },\n",
    "  # We don't allow OpenAI to store the reasoning, you must store it yourself.\n",
    "  store=False,\n",
    "  include=[\"reasoning.encrypted_content\"]\n",
    ")\n",
    "\n",
    "print('#1 First output')\n",
    "for o in response_1.output:\n",
    "    print(o.type,o.to_dict())\n",
    "\n",
    "inputs.extend(response_1.output)\n",
    "inputs.append({  \"role\": \"user\", \"content\": \"Can you summarize it in one sentence?\" })\n",
    "\n",
    "response_2 = await client.responses.create(\n",
    "  model=\"o4-mini\",\n",
    "  reasoning= {\"effort\":\"low\"},\n",
    "  input=inputs,\n",
    "  include=['reasoning.encrypted_content'],\n",
    "  store=False,\n",
    ")\n",
    "\n",
    "print('#2 Second output')\n",
    "for o in response_2.output:\n",
    "    print(o.type,o.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fb1e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9631aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.agents import AzureResponsesAgent\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureOpenAISettings, OpenAISettings\n",
    "\n",
    "\n",
    "# Set up the client and model using Azure OpenAI Resources\n",
    "\n",
    "# Create the AzureResponsesAgent instance using the client and the model\n",
    "agent = AzureResponsesAgent(\n",
    "    ai_model_id=\"o4-mini\",  # Replace with your model deployment name\n",
    "    client=client,\n",
    "    instructions=\"You must always respond in rhymes\",\n",
    "    name=\"RhymingAgent\",\n",
    "    store_enabled=False,  # Set to True if you want to store the responses in Azure OpenAI Responses API\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85ca1929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'My name is John Douglas.'\n",
      "# RhymingAgent: Hello John Douglas, pleased to meet you today,  \n",
      "Your name rings clearly in a charming display.  \n",
      "Tell me your wishes, your queries, your aim—  \n",
      "I’ll rhyme you an answer to brighten your name.\n",
      "\n",
      "# User: 'What is my surname?'\n",
      "# RhymingAgent: Your surname, John, is Douglas through and through,  \n",
      "A name so sturdy, noble, and true.  \n",
      "From start to end it proudly rings,  \n",
      "Douglas it is—now sweetly sings!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "USER_INPUTS = [\n",
    "    \"My name is John Douglas.\",\n",
    "    \"What is my surname?\",\n",
    "]\n",
    "\n",
    "thread = None\n",
    "\n",
    "# Generate the agent response(s)\n",
    "for user_input in USER_INPUTS:\n",
    "    print(f\"# User: '{user_input}'\")\n",
    "    # Invoke the agent for the current message and print the response\n",
    "    response = await agent.get_response(messages=user_input, thread=thread)\n",
    "    print(f\"# {response.name}: {response.content}\")\n",
    "    print()\n",
    "    # Update the thread so the previous response id is used\n",
    "    thread = response.thread\n",
    "\n",
    "# Delete the thread when it is no longer needed\n",
    "# await thread.delete() if thread else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2fe958",
   "metadata": {},
   "source": [
    "Rest of the code same as - https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/agent-types/responses-agent?pivots=programming-language-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aae8ce",
   "metadata": {},
   "source": [
    "## Handle Intermediate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ff098d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /home/agangwal/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages (1.99.8)\n",
      "Collecting openai\n",
      "  Downloading openai-1.99.9-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/agangwal/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/agangwal/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/agangwal/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/agangwal/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/agangwal/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /home/agangwal/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/agangwal/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/agangwal/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /home/agangwal/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /home/agangwal/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /home/agangwal/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/agangwal/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/agangwal/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/agangwal/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/agangwal/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Downloading openai-1.99.9-py3-none-any.whl (786 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m786.8/786.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.99.8\n",
      "    Uninstalling openai-1.99.8:\n",
      "      Successfully uninstalled openai-1.99.8\n",
      "Successfully installed openai-1.99.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efdc522e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10th prime number is 29.\n",
      "\n",
      "How we know:\n",
      "- List the primes in order: 2, 3, 5, 7, 11, 13, 17, 19, 23, 29.\n",
      "- A prime is a number greater than 1 with no positive divisors other than 1 and itself.\n",
      "- Counting them shows 29 is the 10th.\n",
      "- Quick check: 29 isn’t divisible by any prime ≤ √29 (i.e., 2, 3, 5), so it’s prime.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "## NOTE: Semantick Kernel needs AsyncOpenAI client, OpenAI client is not supported.\n",
    "client = AsyncOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_GPT_5_API_KEY\"),\n",
    "    base_url=\"https://learn5858175028.cognitiveservices.azure.com/openai/v1/\", ## https://<just-replace-this>.cognitiveservices.azure.com/openai/v1/\n",
    "    default_query={\"api-version\": \"preview\"}, \n",
    ")\n",
    "\n",
    "response = await client.responses.create(   \n",
    "  model=\"gpt-5\", # Replace with your **model deployment name**\n",
    "  input=f\"What is the 10th prime number? How do you know?\",\n",
    "  timeout=30,\n",
    "  reasoning={\n",
    "    \"effort\": \"low\",  # Options: low, medium, high\n",
    "    \"summary\": \"detailed\" # One of auto, concise, or detailed\n",
    "  },\n",
    "  # previous_response_id=response.id,  # Use the previous response ID if you want to continue the conversation\n",
    ")\n",
    "\n",
    "# print(response.model_dump_json(indent=2))\n",
    "print(response.output[-1].content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3b8369d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseReasoningItem(id='rs_689f02b8f98881a08a2915b40e1375250f22fe94bb8f8d6d', summary=[Summary(text='**Explaining the 10th Prime Number**\\n\\nTo find the 10th prime number, we identify the sequence of prime numbers, which are: 2, 3, 5, 7, 11, 13, 17, 19, 23, and finally 29. A prime number is defined as a number greater than 1 that has no positive divisors other than 1 and itself. While counting, we skip even numbers and multiples of 3. By checking, we verify that 29 is indeed a prime number.', type='summary_text')], type='reasoning', content=None, encrypted_content=None, status=None),\n",
       " ResponseOutputMessage(id='msg_689f02bd7d2481a09ef8977c9b84962a0f22fe94bb8f8d6d', content=[ResponseOutputText(annotations=[], text='The 10th prime number is 29.\\n\\nHow we know:\\n- List the primes in order: 2, 3, 5, 7, 11, 13, 17, 19, 23, 29.\\n- A prime is a number greater than 1 with no positive divisors other than 1 and itself.\\n- Counting them shows 29 is the 10th.\\n- Quick check: 29 isn’t divisible by any prime ≤ √29 (i.e., 2, 3, 5), so it’s prime.', type='output_text', logprobs=None)], role='assistant', status='completed', type='message')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78704888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY\n",
      "**Explaining the 10th Prime Number**\n",
      "\n",
      "To find the 10th prime number, we identify the sequence of prime numbers, which are: 2, 3, 5, 7, 11, 13, 17, 19, 23, and finally 29. A prime number is defined as a number greater than 1 that has no positive divisors other than 1 and itself. While counting, we skip even numbers and multiples of 3. By checking, we verify that 29 is indeed a prime number.\n",
      "CONTENT\n",
      "The 10th prime number is 29.\n",
      "\n",
      "How we know:\n",
      "- List the primes in order: 2, 3, 5, 7, 11, 13, 17, 19, 23, 29.\n",
      "- A prime is a number greater than 1 with no positive divisors other than 1 and itself.\n",
      "- Counting them shows 29 is the 10th.\n",
      "- Quick check: 29 isn’t divisible by any prime ≤ √29 (i.e., 2, 3, 5), so it’s prime.\n"
     ]
    }
   ],
   "source": [
    "for output in response.output:\n",
    "    if hasattr(output, \"summary\"):\n",
    "        print(\"SUMMARY\")\n",
    "        for s in output.summary:\n",
    "            print(s.text)\n",
    "        \n",
    "    if hasattr(output, \"content\") and output.content:\n",
    "        print(\"CONTENT\")\n",
    "        for c in output.content:\n",
    "            print(c.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86935f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# AuthorRole.USER: 'Hello'\n",
      "# Host: Hi there! How can I help with the menu today? Would you like to hear today’s specials or check the price of a specific item?\n",
      "# AuthorRole.USER: 'What is the special soup?'\n",
      "Function Call:> MenuPlugin-get_specials with arguments: {}\n",
      "Function Result:> \n",
      "        Special Soup: Clam Chowder\n",
      "        Special Salad: Cobb Salad\n",
      "        Special Drink: Chai Tea\n",
      "         for function: MenuPlugin-get_specials\n",
      "Function Call:> MenuPlugin-get_specials with arguments: {}\n",
      "Function Result:> \n",
      "        Special Soup: Clam Chowder\n",
      "        Special Salad: Cobb Salad\n",
      "        Special Drink: Chai Tea\n",
      "         for function: MenuPlugin-get_specials\n",
      "Function Call:> MenuPlugin-get_specials with arguments: {}\n",
      "Function Result:> \n",
      "        Special Soup: Clam Chowder\n",
      "        Special Salad: Cobb Salad\n",
      "        Special Drink: Chai Tea\n",
      "         for function: MenuPlugin-get_specials\n",
      "Function Call:> MenuPlugin-get_specials with arguments: {}\n",
      "Function Result:> \n",
      "        Special Soup: Clam Chowder\n",
      "        Special Salad: Cobb Salad\n",
      "        Special Drink: Chai Tea\n",
      "         for function: MenuPlugin-get_specials\n",
      "Function Call:> MenuPlugin-get_specials with arguments: {}\n",
      "Function Result:> \n",
      "        Special Soup: Clam Chowder\n",
      "        Special Salad: Cobb Salad\n",
      "        Special Drink: Chai Tea\n",
      "         for function: MenuPlugin-get_specials\n",
      "# Host: \n",
      "# AuthorRole.USER: 'What is the special drink?'\n"
     ]
    },
    {
     "ename": "AgentExecutionException",
     "evalue": "(\"<class 'semantic_kernel.agents.open_ai.azure_responses_agent.AzureResponsesAgent'> failed to complete the request\", BadRequestError(\"Error code: 400 - {'error': {'message': 'No tool output found for function call call_jSGX3NXXhVeIMTHzSHGeQdRr.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/semantic-kernel/python/semantic_kernel/agents/open_ai/responses_agent_thread_actions.py:495\u001b[39m, in \u001b[36mResponsesAgentThreadActions._get_response\u001b[39m\u001b[34m(cls, agent, chat_history, merged_instructions, previous_response_id, store_output_enabled, tools, response_options, stream)\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     response: Response = \u001b[38;5;28;01mawait\u001b[39;00m agent.client.responses.create(\n\u001b[32m    496\u001b[39m         \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28mcls\u001b[39m._prepare_chat_history_for_request(chat_history),\n\u001b[32m    497\u001b[39m         instructions=merged_instructions \u001b[38;5;129;01mor\u001b[39;00m agent.instructions,\n\u001b[32m    498\u001b[39m         previous_response_id=previous_response_id,\n\u001b[32m    499\u001b[39m         store=store_output_enabled,\n\u001b[32m    500\u001b[39m         tools=tools,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    501\u001b[39m         stream=stream,\n\u001b[32m    502\u001b[39m         **response_options,\n\u001b[32m    503\u001b[39m     )\n\u001b[32m    504\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m BadRequestError \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/openai/resources/responses/responses.py:2161\u001b[39m, in \u001b[36mAsyncResponses.create\u001b[39m\u001b[34m(self, background, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2125\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2126\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2127\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2159\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   2160\u001b[39m ) -> Response | AsyncStream[ResponseStreamEvent]:\n\u001b[32m-> \u001b[39m\u001b[32m2161\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2162\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/responses\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2163\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2164\u001b[39m             {\n\u001b[32m   2165\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mbackground\u001b[39m\u001b[33m\"\u001b[39m: background,\n\u001b[32m   2166\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m: include,\n\u001b[32m   2167\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2168\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minstructions\u001b[39m\u001b[33m\"\u001b[39m: instructions,\n\u001b[32m   2169\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_output_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_output_tokens,\n\u001b[32m   2170\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: max_tool_calls,\n\u001b[32m   2171\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2172\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2173\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2174\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprevious_response_id\u001b[39m\u001b[33m\"\u001b[39m: previous_response_id,\n\u001b[32m   2175\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt,\n\u001b[32m   2176\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2177\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m: reasoning,\n\u001b[32m   2178\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2179\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2180\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2181\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2182\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2183\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2184\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: text,\n\u001b[32m   2185\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2186\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2187\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2188\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2189\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtruncation\u001b[39m\u001b[33m\"\u001b[39m: truncation,\n\u001b[32m   2190\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2191\u001b[39m             },\n\u001b[32m   2192\u001b[39m             response_create_params.ResponseCreateParamsStreaming\n\u001b[32m   2193\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2194\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m response_create_params.ResponseCreateParamsNonStreaming,\n\u001b[32m   2195\u001b[39m         ),\n\u001b[32m   2196\u001b[39m         options=make_request_options(\n\u001b[32m   2197\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2198\u001b[39m         ),\n\u001b[32m   2199\u001b[39m         cast_to=Response,\n\u001b[32m   2200\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2201\u001b[39m         stream_cls=AsyncStream[ResponseStreamEvent],\n\u001b[32m   2202\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/openai/_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1791\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/openai/_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1593\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': 'No tool output found for function call call_jSGX3NXXhVeIMTHzSHGeQdRr.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAgentExecutionException\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     75\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m thread.delete() \u001b[38;5;28;01mif\u001b[39;00m thread \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m user_input \u001b[38;5;129;01min\u001b[39;00m user_inputs:\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m# \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAuthorRole.USER\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m agent.invoke(\n\u001b[32m     67\u001b[39m         messages=user_input,\n\u001b[32m     68\u001b[39m         thread=thread,\n\u001b[32m     69\u001b[39m         on_intermediate_message=handle_intermediate_steps,\n\u001b[32m     70\u001b[39m     ):\n\u001b[32m     71\u001b[39m         thread = response.thread\n\u001b[32m     72\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m# \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/semantic-kernel/python/semantic_kernel/utils/telemetry/agent_diagnostics/decorators.py:106\u001b[39m, in \u001b[36mtrace_agent_invocation.<locals>.wrapper_decorator\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(invoke_func)\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper_decorator\u001b[39m(\n\u001b[32m    101\u001b[39m     *args: P.args,\n\u001b[32m    102\u001b[39m     **kwargs: P.kwargs,\n\u001b[32m    103\u001b[39m ) -> AsyncIterable[AgentResponseItem[ChatMessageContent]]:\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m are_model_diagnostics_enabled():\n\u001b[32m    105\u001b[39m         \u001b[38;5;66;03m# If model diagnostics are not enabled, just return the responses\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m invoke_func(*args, **kwargs):\n\u001b[32m    107\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m response\n\u001b[32m    108\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/semantic-kernel/python/semantic_kernel/agents/open_ai/openai_responses_agent.py:1024\u001b[39m, in \u001b[36mOpenAIResponsesAgent.invoke\u001b[39m\u001b[34m(self, messages, thread, on_intermediate_message, arguments, kernel, function_choice_behavior, include, instructions_override, max_output_tokens, metadata, model, parallel_tool_calls, polling_options, reasoning, temperature, text, tools, top_p, truncation, **kwargs)\u001b[39m\n\u001b[32m   1021\u001b[39m function_choice_behavior = function_choice_behavior \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_choice_behavior\n\u001b[32m   1022\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m function_choice_behavior \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# nosec\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m is_visible, message \u001b[38;5;129;01min\u001b[39;00m ResponsesAgentThreadActions.invoke(\n\u001b[32m   1025\u001b[39m     agent=\u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1026\u001b[39m     chat_history=chat_history,\n\u001b[32m   1027\u001b[39m     thread=thread,\n\u001b[32m   1028\u001b[39m     store_enabled=\u001b[38;5;28mself\u001b[39m.store_enabled,\n\u001b[32m   1029\u001b[39m     kernel=kernel,\n\u001b[32m   1030\u001b[39m     arguments=arguments,\n\u001b[32m   1031\u001b[39m     function_choice_behavior=function_choice_behavior,\n\u001b[32m   1032\u001b[39m     **response_level_params,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1033\u001b[39m ):\n\u001b[32m   1034\u001b[39m     message.metadata[\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m] = thread.id\n\u001b[32m   1035\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m thread.on_new_message(message)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/semantic-kernel/python/semantic_kernel/agents/open_ai/responses_agent_thread_actions.py:176\u001b[39m, in \u001b[36mResponsesAgentThreadActions.invoke\u001b[39m\u001b[34m(cls, agent, chat_history, thread, store_enabled, function_choice_behavior, arguments, include, instructions_override, kernel, max_output_tokens, metadata, model, parallel_tool_calls, polling_options, reasoning, text, tools, temperature, top_p, truncation, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m     previous_response_id = thread.response_id\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m request_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(function_choice_behavior.maximum_auto_invoke_attempts):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_response(\n\u001b[32m    177\u001b[39m         agent=agent,\n\u001b[32m    178\u001b[39m         chat_history=override_history,\n\u001b[32m    179\u001b[39m         merged_instructions=merged_instructions,\n\u001b[32m    180\u001b[39m         previous_response_id=previous_response_id,\n\u001b[32m    181\u001b[39m         store_output_enabled=store_enabled,\n\u001b[32m    182\u001b[39m         tools=tools,\n\u001b[32m    183\u001b[39m         response_options=response_options,\n\u001b[32m    184\u001b[39m     )\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, Response):\n\u001b[32m    186\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m AgentInvokeException(\u001b[33m\"\u001b[39m\u001b[33mResponse is not of type Response\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/semantic-kernel/python/semantic_kernel/agents/open_ai/responses_agent_thread_actions.py:510\u001b[39m, in \u001b[36mResponsesAgentThreadActions._get_response\u001b[39m\u001b[34m(cls, agent, chat_history, merged_instructions, previous_response_id, store_output_enabled, tools, response_options, stream)\u001b[39m\n\u001b[32m    505\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ex.code == \u001b[33m\"\u001b[39m\u001b[33mcontent_filter\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    506\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ContentFilterAIException(\n\u001b[32m    507\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(agent)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m encountered a content error\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    508\u001b[39m             ex,\n\u001b[32m    509\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m AgentExecutionException(\n\u001b[32m    511\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(agent)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m failed to complete the request\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    512\u001b[39m         ex,\n\u001b[32m    513\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n\u001b[32m    514\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m    515\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m AgentExecutionException(\n\u001b[32m    516\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(agent)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m service failed to complete the request\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    517\u001b[39m         ex,\n\u001b[32m    518\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n",
      "\u001b[31mAgentExecutionException\u001b[39m: (\"<class 'semantic_kernel.agents.open_ai.azure_responses_agent.AzureResponsesAgent'> failed to complete the request\", BadRequestError(\"Error code: 400 - {'error': {'message': 'No tool output found for function call call_jSGX3NXXhVeIMTHzSHGeQdRr.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}\"))"
     ]
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from semantic_kernel.agents import AzureResponsesAgent\n",
    "from semantic_kernel.contents import AuthorRole, FunctionCallContent, FunctionResultContent\n",
    "from semantic_kernel.contents.chat_message_content import ChatMessageContent\n",
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "\n",
    "# Define a sample plugin for the sample\n",
    "class MenuPlugin:\n",
    "    \"\"\"A sample Menu Plugin used for the concept sample.\"\"\"\n",
    "\n",
    "    @kernel_function(description=\"Provides a list of specials from the menu.\")\n",
    "    def get_specials(self) -> Annotated[str, \"Returns the specials from the menu.\"]:\n",
    "        return \"\"\"\n",
    "        Special Soup: Clam Chowder\n",
    "        Special Salad: Cobb Salad\n",
    "        Special Drink: Chai Tea\n",
    "        \"\"\"\n",
    "\n",
    "    @kernel_function(description=\"Provides the price of the requested menu item.\")\n",
    "    def get_item_price(\n",
    "        self, menu_item: Annotated[str, \"The name of the menu item.\"]\n",
    "    ) -> Annotated[str, \"Returns the price of the menu item.\"]:\n",
    "        return \"$9.99\"\n",
    "\n",
    "\n",
    "MESSAGES = []\n",
    "async def handle_intermediate_steps(message: ChatMessageContent) -> None:\n",
    "    MESSAGES.append(message)\n",
    "    for item in message.items or []:\n",
    "        if isinstance(item, FunctionResultContent):\n",
    "            print(f\"Function Result:> {item.result} for function: {item.name}\")\n",
    "        elif isinstance(item, FunctionCallContent):\n",
    "            print(f\"Function Call:> {item.name} with arguments: {item.arguments}\")\n",
    "        else:\n",
    "            print(f\"{item}\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    # 1. Create the client using Azure OpenAI resources and configuration\n",
    "    # client = AzureResponsesAgent.create_client()\n",
    "\n",
    "    # 2. Create a Semantic Kernel agent for the OpenAI Responses API\n",
    "    agent = AzureResponsesAgent(\n",
    "        ai_model_id=\"gpt-5\",  # Replace with your model deployment name\n",
    "        client=client,\n",
    "        instructions=\"Answer questions about the menu.\",\n",
    "        name=\"Host\",\n",
    "        plugins=[MenuPlugin()],\n",
    "        reasoning_effort=\"high\",  # Set the reasoning effort level\n",
    "        store_enabled=False,  # Set to True if you want to store the responses in Azure\n",
    "    )\n",
    "\n",
    "\n",
    "    # 3. Create a thread for the agent\n",
    "    # If no thread is provided, a new thread will be\n",
    "    # created and returned with the initial response\n",
    "    thread = None\n",
    "\n",
    "    user_inputs = [\"Hello\", \"What is the special soup?\", \"What is the special drink?\", \"How much is that?\", \"Thank you\"]\n",
    "\n",
    "    try:\n",
    "        for user_input in user_inputs:\n",
    "            print(f\"# {AuthorRole.USER}: '{user_input}'\")\n",
    "            async for response in agent.invoke(\n",
    "                messages=user_input,\n",
    "                thread=thread,\n",
    "                on_intermediate_message=handle_intermediate_steps,\n",
    "            ):\n",
    "                thread = response.thread\n",
    "                print(f\"# {response.name}: {response.content}\")\n",
    "                \n",
    "    finally:\n",
    "        await thread.delete() if thread else None\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7407661a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:09 --:--:--     0curl: (6) Could not resolve host: lseg-foundry-demo\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\ncurl -X POST https://lseg-foundry-demo/openai/deployments/gpt-5/completions?api-version=2024-10-21 \\\\\\n  -H \"Content-Type: application/json\" \\\\\\n  -H \"api-key: $AZURE_REASONING_API_KEY\" \\\\\\n  -d \\'{\\n     \"prompt\": \"This is a test\"\\n    }\\'\\n'' returned non-zero exit status 6.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msh\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mcurl -X POST https://lseg-foundry-demo/openai/deployments/gpt-5/completions?api-version=2024-10-21 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  -H \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mContent-Type: application/json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  -H \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapi-key: $AZURE_REASONING_API_KEY\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  -d \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m     \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mThis is a test\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    }\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2565\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2564\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2565\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2568\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2569\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/IPython/core/magics/script.py:160\u001b[39m, in \u001b[36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[39m\u001b[34m(line, cell)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    159\u001b[39m     line = script\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/IPython/core/magics/script.py:343\u001b[39m, in \u001b[36mScriptMagics.shebang\u001b[39m\u001b[34m(self, line, cell)\u001b[39m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.raise_error \u001b[38;5;129;01mand\u001b[39;00m p.returncode != \u001b[32m0\u001b[39m:\n\u001b[32m    339\u001b[39m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[32m    340\u001b[39m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[32m    341\u001b[39m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[32m    342\u001b[39m     rc = p.returncode \u001b[38;5;129;01mor\u001b[39;00m -\u001b[32m9\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command 'b'\\ncurl -X POST https://lseg-foundry-demo/openai/deployments/gpt-5/completions?api-version=2024-10-21 \\\\\\n  -H \"Content-Type: application/json\" \\\\\\n  -H \"api-key: $AZURE_REASONING_API_KEY\" \\\\\\n  -d \\'{\\n     \"prompt\": \"This is a test\"\\n    }\\'\\n'' returned non-zero exit status 6."
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "curl -X POST https://lseg-foundry-demo./openai/deployments/gpt-5/completions?api-version=2024-10-21 \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"api-key: $AZURE_REASONING_API_KEY\" \\\n",
    "  -d '{\n",
    "     \"prompt\": \"This is a test\"\n",
    "    }'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8539a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    61    0     0  100    61      0      2  0:00:30  0:00:20  0:00:10     0"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\ncurl -X POST https://lseg-foundry-demo.openai.azure.com/openai/v1/responses?api-version=preview \\\\\\n  -H \"Content-Type: application/json\" \\\\\\n  -H \"api-key: $AZURE_REASONING_API_KEY\" \\\\\\n  -d \\'{\\n     \"model\": \"gpt-5\",\\n     \"input\": \"This is a test\"\\n    }\\'\\n'' died with <Signals.SIGINT: 2>.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msh\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mcurl -X POST https://lseg-foundry-demo.openai.azure.com/openai/v1/responses?api-version=preview \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  -H \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mContent-Type: application/json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  -H \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapi-key: $AZURE_REASONING_API_KEY\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  -d \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m     \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m     \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mThis is a test\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    }\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2565\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2564\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2565\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2568\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2569\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/IPython/core/magics/script.py:160\u001b[39m, in \u001b[36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[39m\u001b[34m(line, cell)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    159\u001b[39m     line = script\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/IPython/core/magics/script.py:334\u001b[39m, in \u001b[36mScriptMagics.shebang\u001b[39m\u001b[34m(self, line, cell)\u001b[39m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mError while terminating subprocess (pid=\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (p.pid, e))\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.raise_error:\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(p.returncode, cell) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command 'b'\\ncurl -X POST https://lseg-foundry-demo.openai.azure.com/openai/v1/responses?api-version=preview \\\\\\n  -H \"Content-Type: application/json\" \\\\\\n  -H \"api-key: $AZURE_REASONING_API_KEY\" \\\\\\n  -d \\'{\\n     \"model\": \"gpt-5\",\\n     \"input\": \"This is a test\"\\n    }\\'\\n'' died with <Signals.SIGINT: 2>."
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "curl -X POST https://lseg-foundry-demo.openai.azure.com/openai/v1/responses?api-version=preview \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"api-key: $AZURE_REASONING_API_KEY\" \\\n",
    "  -d '{\n",
    "     \"model\": \"gpt-5\",\n",
    "     \"input\": \"This is a test\"\n",
    "    }'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44fb85b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_normal = OpenAI(\n",
    "    api_key=os.getenv(\"AZURE_REASONING_API_KEY\"),\n",
    "    base_url=\"https://lseg-foundry-demo.cognitiveservices.azure.com/openai/v1/\", ## https://<just-replace-this>.cognitiveservices.azure.com/openai/v1/\n",
    "    default_query={\"api-version\": \"preview\"}, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11425676",
   "metadata": {},
   "outputs": [
    {
     "ename": "APITimeoutError",
     "evalue": "Request timed out.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:126\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    125\u001b[39m exc_map: ExceptionMapping = {socket.timeout: ReadTimeout, \u001b[38;5;167;01mOSError\u001b[39;00m: ReadError}\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43msettimeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/httpcore/_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mReadTimeout\u001b[39m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/openai/_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:249\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mReadTimeout\u001b[39m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAPITimeoutError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43mclient_normal\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m   \u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Replace with your **model deployment name**\u001b[39;49;00m\n\u001b[32m      3\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhats the capital of france?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m  \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# reasoning={\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#   \"effort\": \"low\",  # Options: low, medium, high\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#   # \"summary\": \"detailed\" # One of auto, concise, or detailed\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# },\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# previous_response_id=response.id,  # Use the previous response ID if you want to continue the conversation\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# print(response.model_dump_json(indent=2))\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.output[-\u001b[32m1\u001b[39m].content[\u001b[32m0\u001b[39m].text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/openai/resources/responses/responses.py:795\u001b[39m, in \u001b[36mResponses.create\u001b[39m\u001b[34m(self, background, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    759\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    760\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    761\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    793\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    794\u001b[39m ) -> Response | Stream[ResponseStreamEvent]:\n\u001b[32m--> \u001b[39m\u001b[32m795\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/responses\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackground\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstructions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_output_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprevious_response_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    813\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    814\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    816\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    817\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    819\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    821\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtruncation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParamsStreaming\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lseg-migration-agent/migration-agent/.venv/lib/python3.12/site-packages/openai/_base_client.py:1000\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    997\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    999\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising timeout error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1000\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APITimeoutError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   1002\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered Exception\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mAPITimeoutError\u001b[39m: Request timed out."
     ]
    }
   ],
   "source": [
    "response = client_normal.responses.create(   \n",
    "  model=\"gpt-5\", # Replace with your **model deployment name**\n",
    "  input=f\"whats the capital of france?\",\n",
    "  timeout=10\n",
    "  # reasoning={\n",
    "  #   \"effort\": \"low\",  # Options: low, medium, high\n",
    "  #   # \"summary\": \"detailed\" # One of auto, concise, or detailed\n",
    "  # },\n",
    "  # previous_response_id=response.id,  # Use the previous response ID if you want to continue the conversation\n",
    ")\n",
    "\n",
    "# print(response.model_dump_json(indent=2))\n",
    "print(response.output[-1].content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fa1dcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.35.0\n"
     ]
    }
   ],
   "source": [
    "import semantic_kernel as sk\n",
    "\n",
    "# print version of semantic kernel\n",
    "print(sk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba4574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "AgentExecutionException: (\"<class 'semantic_kernel.agents.open_ai.azure_responses_agent.AzureResponsesAgent'> failed to complete the request\", BadRequestError('Error code: 400 - {\\'error\\': {\\'message\\': \"Item \\'fc_688ca90681b881909f2c5b371eb9e6af0d16c1745b70df7c\\' of type \\'function_call\\' was provided without its required \\'reasoning\\' item: \\'rs_688ca905f62c8190b259a6183fa53f7f0d16c1745b70df7c\\'.\", \\'type\\': \\'invalid_request_error\\', \\'param\\': \\'input\\', \\'code\\': None}}'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
